{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "444816ae-cc57-4dc3-9baa-a5f3cb05f812",
   "metadata": {},
   "source": [
    "## MNIST with Torch - Covolutional\n",
    "In this exercise, I try to use a CNN for MNIST with pytorch and pytorch-vision dataset from MNIST. The Neural Network and the train method are in the archives of this paste."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f810c499-4ccc-468d-9a24-244e4f2de42b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "from torchvision import datasets, transforms\n",
    "from neural_network import MNIST_CNN\n",
    "from train import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2b0e109a-cd28-49e1-b528-f9bc744d3ac3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Runing in cpu\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Runing in {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee8082ea-a62b-48ec-907e-54b9a4fbf42b",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "773b3434-fb7b-4192-b564-87c57cca733e",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "nsamples = 50\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor() \n",
    "])\n",
    "\n",
    "dataset1 = datasets.MNIST('../data', train=True, download=True,transform=transform)\n",
    "dataset2 = datasets.MNIST('../data', train=False,transform=transform)\n",
    "train_loader = torch.utils.data.DataLoader(dataset1,batch_size=nsamples, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(dataset2,batch_size=nsamples, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dfc13433-9ae0-4385-88f4-9c47d86c6d67",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MNIST_CNN().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e3dc9438-a929-4db8-99b8-f9e02373359c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1:\n",
      "Imagens (data): torch.Size([50, 1, 28, 28])\n",
      "Rótulos (target): torch.Size([50])\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "for batch_idx, (data, target) in enumerate(train_loader):\n",
    "    print(f'Batch {batch_idx + 1}:')\n",
    "    print('Imagens (data):', data.size())\n",
    "    print('Rótulos (target):', target.size())\n",
    "\n",
    "\n",
    "    plt.imshow(data[0].squeeze(), cmap='gray')\n",
    "    plt.title(f'Rótulo: {target[0].item()}')\n",
    "    plt.show()\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "94983269-92c6-4467-baf9-b3a0f36e3eb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 \n",
      " Loss: 2.3026845455169678 \n",
      " Acurracy: 6.666666740784422e-05\n",
      "Train Epoch: 10 \n",
      " Loss: 2.300323486328125 \n",
      " Acurracy: 0.0011666666250675917\n",
      "Train Epoch: 20 \n",
      " Loss: 2.291909694671631 \n",
      " Acurracy: 0.0024666667450219393\n",
      "Train Epoch: 30 \n",
      " Loss: 2.2623488903045654 \n",
      " Acurracy: 0.004749999847263098\n",
      "Train Epoch: 40 \n",
      " Loss: 2.1464550495147705 \n",
      " Acurracy: 0.007600000128149986\n",
      "Train Epoch: 50 \n",
      " Loss: 2.0462145805358887 \n",
      " Acurracy: 0.01138333324342966\n",
      "Train Epoch: 60 \n",
      " Loss: 1.9360296726226807 \n",
      " Acurracy: 0.016249999403953552\n",
      "Train Epoch: 70 \n",
      " Loss: 1.779322624206543 \n",
      " Acurracy: 0.021833334118127823\n",
      "Train Epoch: 80 \n",
      " Loss: 1.7311385869979858 \n",
      " Acurracy: 0.02801666595041752\n",
      "Train Epoch: 90 \n",
      " Loss: 1.721813678741455 \n",
      " Acurracy: 0.03413333371281624\n",
      "Train Epoch: 100 \n",
      " Loss: 1.712072491645813 \n",
      " Acurracy: 0.04066666588187218\n",
      "Train Epoch: 110 \n",
      " Loss: 1.6602041721343994 \n",
      " Acurracy: 0.04743333160877228\n",
      "Train Epoch: 120 \n",
      " Loss: 1.6513253450393677 \n",
      " Acurracy: 0.053833331912755966\n",
      "Train Epoch: 130 \n",
      " Loss: 1.6790889501571655 \n",
      " Acurracy: 0.06043333187699318\n",
      "Train Epoch: 140 \n",
      " Loss: 1.5951910018920898 \n",
      " Acurracy: 0.06750000268220901\n",
      "Train Epoch: 150 \n",
      " Loss: 1.6950660943984985 \n",
      " Acurracy: 0.07418332993984222\n",
      "Train Epoch: 160 \n",
      " Loss: 1.6698271036148071 \n",
      " Acurracy: 0.08079999685287476\n",
      "Train Epoch: 170 \n",
      " Loss: 1.6335210800170898 \n",
      " Acurracy: 0.08801666647195816\n",
      "Train Epoch: 180 \n",
      " Loss: 1.5861787796020508 \n",
      " Acurracy: 0.09510000050067902\n",
      "Train Epoch: 190 \n",
      " Loss: 1.63380765914917 \n",
      " Acurracy: 0.10206666588783264\n",
      "Train Epoch: 200 \n",
      " Loss: 1.6489142179489136 \n",
      " Acurracy: 0.10896666347980499\n",
      "Train Epoch: 210 \n",
      " Loss: 1.6673307418823242 \n",
      " Acurracy: 0.11576666682958603\n",
      "Train Epoch: 220 \n",
      " Loss: 1.5813113451004028 \n",
      " Acurracy: 0.12280000001192093\n",
      "Train Epoch: 230 \n",
      " Loss: 1.6343934535980225 \n",
      " Acurracy: 0.12974999845027924\n",
      "Train Epoch: 240 \n",
      " Loss: 1.5768734216690063 \n",
      " Acurracy: 0.13713333010673523\n",
      "Train Epoch: 250 \n",
      " Loss: 1.6028529405593872 \n",
      " Acurracy: 0.14428333938121796\n",
      "Train Epoch: 260 \n",
      " Loss: 1.6061419248580933 \n",
      " Acurracy: 0.15138334035873413\n",
      "Train Epoch: 270 \n",
      " Loss: 1.597438931465149 \n",
      " Acurracy: 0.15835000574588776\n",
      "Train Epoch: 280 \n",
      " Loss: 1.5646852254867554 \n",
      " Acurracy: 0.16556666791439056\n",
      "Train Epoch: 290 \n",
      " Loss: 1.6396076679229736 \n",
      " Acurracy: 0.17288333177566528\n",
      "Train Epoch: 300 \n",
      " Loss: 1.5184516906738281 \n",
      " Acurracy: 0.18041667342185974\n",
      "Train Epoch: 310 \n",
      " Loss: 1.5379163026809692 \n",
      " Acurracy: 0.18778333067893982\n",
      "Train Epoch: 320 \n",
      " Loss: 1.5617414712905884 \n",
      " Acurracy: 0.19518333673477173\n",
      "Train Epoch: 330 \n",
      " Loss: 1.6541141271591187 \n",
      " Acurracy: 0.20243333280086517\n",
      "Train Epoch: 340 \n",
      " Loss: 1.5492140054702759 \n",
      " Acurracy: 0.20983333885669708\n",
      "Train Epoch: 350 \n",
      " Loss: 1.590095043182373 \n",
      " Acurracy: 0.21738334000110626\n",
      "Train Epoch: 360 \n",
      " Loss: 1.5301262140274048 \n",
      " Acurracy: 0.22499999403953552\n",
      "Train Epoch: 370 \n",
      " Loss: 1.5275501012802124 \n",
      " Acurracy: 0.2326499968767166\n",
      "Train Epoch: 380 \n",
      " Loss: 1.5643383264541626 \n",
      " Acurracy: 0.2398499995470047\n",
      "Train Epoch: 390 \n",
      " Loss: 1.5491228103637695 \n",
      " Acurracy: 0.2472500056028366\n",
      "Train Epoch: 400 \n",
      " Loss: 1.5423742532730103 \n",
      " Acurracy: 0.2545166611671448\n",
      "Train Epoch: 410 \n",
      " Loss: 1.609977126121521 \n",
      " Acurracy: 0.26190000772476196\n",
      "Train Epoch: 420 \n",
      " Loss: 1.6780415773391724 \n",
      " Acurracy: 0.26901665329933167\n",
      "Train Epoch: 430 \n",
      " Loss: 1.5759986639022827 \n",
      " Acurracy: 0.27638334035873413\n",
      "Train Epoch: 440 \n",
      " Loss: 1.5280574560165405 \n",
      " Acurracy: 0.28396666049957275\n",
      "Train Epoch: 450 \n",
      " Loss: 1.529021143913269 \n",
      " Acurracy: 0.2915666699409485\n",
      "Train Epoch: 460 \n",
      " Loss: 1.6229252815246582 \n",
      " Acurracy: 0.2990666627883911\n",
      "Train Epoch: 470 \n",
      " Loss: 1.5936012268066406 \n",
      " Acurracy: 0.306466668844223\n",
      "Train Epoch: 480 \n",
      " Loss: 1.5780848264694214 \n",
      " Acurracy: 0.31404998898506165\n",
      "Train Epoch: 490 \n",
      " Loss: 1.5299301147460938 \n",
      " Acurracy: 0.3215999901294708\n",
      "Train Epoch: 500 \n",
      " Loss: 1.5875614881515503 \n",
      " Acurracy: 0.32883334159851074\n",
      "Train Epoch: 510 \n",
      " Loss: 1.572098731994629 \n",
      " Acurracy: 0.3362666666507721\n",
      "Train Epoch: 520 \n",
      " Loss: 1.6484694480895996 \n",
      " Acurracy: 0.34375\n",
      "Train Epoch: 530 \n",
      " Loss: 1.529158115386963 \n",
      " Acurracy: 0.35136666893959045\n",
      "Train Epoch: 540 \n",
      " Loss: 1.5744003057479858 \n",
      " Acurracy: 0.3588833212852478\n",
      "Train Epoch: 550 \n",
      " Loss: 1.5654001235961914 \n",
      " Acurracy: 0.36641666293144226\n",
      "Train Epoch: 560 \n",
      " Loss: 1.5538345575332642 \n",
      " Acurracy: 0.373933345079422\n",
      "Train Epoch: 570 \n",
      " Loss: 1.607438564300537 \n",
      " Acurracy: 0.38136667013168335\n",
      "Train Epoch: 580 \n",
      " Loss: 1.5607273578643799 \n",
      " Acurracy: 0.3888166546821594\n",
      "Train Epoch: 590 \n",
      " Loss: 1.546011209487915 \n",
      " Acurracy: 0.39614999294281006\n",
      "Train Epoch: 600 \n",
      " Loss: 1.5483050346374512 \n",
      " Acurracy: 0.40371665358543396\n",
      "Train Epoch: 610 \n",
      " Loss: 1.5811691284179688 \n",
      " Acurracy: 0.411216676235199\n",
      "Train Epoch: 620 \n",
      " Loss: 1.551771879196167 \n",
      " Acurracy: 0.4188833236694336\n",
      "Train Epoch: 630 \n",
      " Loss: 1.5345324277877808 \n",
      " Acurracy: 0.4266333281993866\n",
      "Train Epoch: 640 \n",
      " Loss: 1.5055458545684814 \n",
      " Acurracy: 0.4343000054359436\n",
      "Train Epoch: 650 \n",
      " Loss: 1.5278973579406738 \n",
      " Acurracy: 0.4420333206653595\n",
      "Train Epoch: 660 \n",
      " Loss: 1.590948462486267 \n",
      " Acurracy: 0.44966667890548706\n",
      "Train Epoch: 670 \n",
      " Loss: 1.605471134185791 \n",
      " Acurracy: 0.4572833478450775\n",
      "Train Epoch: 680 \n",
      " Loss: 1.5501306056976318 \n",
      " Acurracy: 0.46496665477752686\n",
      "Train Epoch: 690 \n",
      " Loss: 1.567421555519104 \n",
      " Acurracy: 0.47269999980926514\n",
      "Train Epoch: 700 \n",
      " Loss: 1.5145007371902466 \n",
      " Acurracy: 0.4804166555404663\n",
      "Train Epoch: 710 \n",
      " Loss: 1.6050522327423096 \n",
      " Acurracy: 0.48803332448005676\n",
      "Train Epoch: 720 \n",
      " Loss: 1.545638918876648 \n",
      " Acurracy: 0.4957166612148285\n",
      "Train Epoch: 730 \n",
      " Loss: 1.5785640478134155 \n",
      " Acurracy: 0.503350019454956\n",
      "Train Epoch: 740 \n",
      " Loss: 1.567577838897705 \n",
      " Acurracy: 0.5109500288963318\n",
      "Train Epoch: 750 \n",
      " Loss: 1.5004948377609253 \n",
      " Acurracy: 0.5188666582107544\n",
      "Train Epoch: 760 \n",
      " Loss: 1.5621206760406494 \n",
      " Acurracy: 0.5265666842460632\n",
      "Train Epoch: 770 \n",
      " Loss: 1.5956299304962158 \n",
      " Acurracy: 0.5340499877929688\n",
      "Train Epoch: 780 \n",
      " Loss: 1.5850199460983276 \n",
      " Acurracy: 0.5416833162307739\n",
      "Train Epoch: 790 \n",
      " Loss: 1.508131742477417 \n",
      " Acurracy: 0.5494499802589417\n",
      "Train Epoch: 800 \n",
      " Loss: 1.6495438814163208 \n",
      " Acurracy: 0.5570666790008545\n",
      "Train Epoch: 810 \n",
      " Loss: 1.5671539306640625 \n",
      " Acurracy: 0.564716637134552\n",
      "Train Epoch: 820 \n",
      " Loss: 1.5629875659942627 \n",
      " Acurracy: 0.5725333094596863\n",
      "Train Epoch: 830 \n",
      " Loss: 1.548262357711792 \n",
      " Acurracy: 0.5802666544914246\n",
      "Train Epoch: 840 \n",
      " Loss: 1.540199637413025 \n",
      " Acurracy: 0.5879833102226257\n",
      "Train Epoch: 850 \n",
      " Loss: 1.5408804416656494 \n",
      " Acurracy: 0.5955666899681091\n",
      "Train Epoch: 860 \n",
      " Loss: 1.5156341791152954 \n",
      " Acurracy: 0.6031833291053772\n",
      "Train Epoch: 870 \n",
      " Loss: 1.490384578704834 \n",
      " Acurracy: 0.6109499931335449\n",
      "Train Epoch: 880 \n",
      " Loss: 1.5106925964355469 \n",
      " Acurracy: 0.6186500191688538\n",
      "Train Epoch: 890 \n",
      " Loss: 1.4869998693466187 \n",
      " Acurracy: 0.6265000104904175\n",
      "Train Epoch: 900 \n",
      " Loss: 1.5063564777374268 \n",
      " Acurracy: 0.6342166662216187\n",
      "Train Epoch: 910 \n",
      " Loss: 1.540358304977417 \n",
      " Acurracy: 0.6419166922569275\n",
      "Train Epoch: 920 \n",
      " Loss: 1.5436210632324219 \n",
      " Acurracy: 0.6496833562850952\n",
      "Train Epoch: 930 \n",
      " Loss: 1.6389394998550415 \n",
      " Acurracy: 0.6574500203132629\n",
      "Train Epoch: 940 \n",
      " Loss: 1.5881686210632324 \n",
      " Acurracy: 0.6650166511535645\n",
      "Train Epoch: 950 \n",
      " Loss: 1.5536121129989624 \n",
      " Acurracy: 0.6727833151817322\n",
      "Train Epoch: 960 \n",
      " Loss: 1.5972163677215576 \n",
      " Acurracy: 0.6805499792098999\n",
      "Train Epoch: 970 \n",
      " Loss: 1.5924378633499146 \n",
      " Acurracy: 0.6883500218391418\n",
      "Train Epoch: 980 \n",
      " Loss: 1.5676066875457764 \n",
      " Acurracy: 0.6962000131607056\n",
      "Train Epoch: 990 \n",
      " Loss: 1.5778851509094238 \n",
      " Acurracy: 0.7039999961853027\n",
      "Train Epoch: 1000 \n",
      " Loss: 1.4886707067489624 \n",
      " Acurracy: 0.7117666602134705\n",
      "Train Epoch: 1010 \n",
      " Loss: 1.5396065711975098 \n",
      " Acurracy: 0.7194166779518127\n",
      "Train Epoch: 1020 \n",
      " Loss: 1.5662237405776978 \n",
      " Acurracy: 0.727150022983551\n",
      "Train Epoch: 1030 \n",
      " Loss: 1.5430313348770142 \n",
      " Acurracy: 0.7348499894142151\n",
      "Train Epoch: 1040 \n",
      " Loss: 1.515541672706604 \n",
      " Acurracy: 0.7425500154495239\n",
      "Train Epoch: 1050 \n",
      " Loss: 1.541805386543274 \n",
      " Acurracy: 0.7504500150680542\n",
      "Train Epoch: 1060 \n",
      " Loss: 1.5617239475250244 \n",
      " Acurracy: 0.7580000162124634\n",
      "Train Epoch: 1070 \n",
      " Loss: 1.519494891166687 \n",
      " Acurracy: 0.76583331823349\n",
      "Train Epoch: 1080 \n",
      " Loss: 1.5510236024856567 \n",
      " Acurracy: 0.7735666632652283\n",
      "Train Epoch: 1090 \n",
      " Loss: 1.49625825881958 \n",
      " Acurracy: 0.7815166711807251\n",
      "Train Epoch: 1100 \n",
      " Loss: 1.4870116710662842 \n",
      " Acurracy: 0.7893499732017517\n",
      "Train Epoch: 1110 \n",
      " Loss: 1.5255283117294312 \n",
      " Acurracy: 0.7969499826431274\n",
      "Train Epoch: 1120 \n",
      " Loss: 1.569583535194397 \n",
      " Acurracy: 0.8048666715621948\n",
      "Train Epoch: 1130 \n",
      " Loss: 1.4739460945129395 \n",
      " Acurracy: 0.812833309173584\n",
      "Train Epoch: 1140 \n",
      " Loss: 1.5347551107406616 \n",
      " Acurracy: 0.8206833600997925\n",
      "Train Epoch: 1150 \n",
      " Loss: 1.537685751914978 \n",
      " Acurracy: 0.8285999894142151\n",
      "Train Epoch: 1160 \n",
      " Loss: 1.5474154949188232 \n",
      " Acurracy: 0.8363333344459534\n",
      "Train Epoch: 1170 \n",
      " Loss: 1.5898308753967285 \n",
      " Acurracy: 0.8439833521842957\n",
      "Train Epoch: 1180 \n",
      " Loss: 1.4896936416625977 \n",
      " Acurracy: 0.8518666625022888\n",
      "Train Epoch: 1190 \n",
      " Loss: 1.5711545944213867 \n",
      " Acurracy: 0.8597333431243896\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "train(model,device,train_loader,optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f75c452e-fa84-4b2a-97f8-1a9be390bd92",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c60935f3-30ae-4da6-8f1a-96e1bc2bd9f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.0303124275803566 \n",
      " Acurracy 0.949\n"
     ]
    }
   ],
   "source": [
    "test(model,device,test_loader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
