{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ad391a51-6a46-490f-a33d-31eac7d1fa12",
   "metadata": {},
   "source": [
    "## MNIST with Torch - Linear\n",
    "In this exercise, I try to use a Linear NN for MNIST with pytorch and pytorch-vision dataset from MNIST. The Neural Network and the train method are in the archives of this paste."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6faaff98-cf7e-4d88-bfdd-dccdb6ed6d18",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "from torchvision import datasets, transforms\n",
    "from neural_network import MNIST_NN_Linear\n",
    "from train import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9a480442-4fc2-44be-8d9a-ae21cc995284",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Runing in cpu\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Runing in {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a8d8336-c6fa-4d8a-9eab-87b6e736ecb0",
   "metadata": {},
   "source": [
    "### Training\n",
    "For a Linear model, I did a transform method to PIL-Image to a Tensor, that will be resize later on the code to a matrix (batch size and 784 for the pixels). The batch size, chosed arbitrary, was 50, because this would make more epoch's to the NN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0266a6b5-6a51-40c4-8604-139f7fad3ef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "nsamples = 50\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor() \n",
    "])\n",
    "\n",
    "dataset1 = datasets.MNIST('../data', train=True, download=True,transform=transform)\n",
    "dataset2 = datasets.MNIST('../data', train=False,transform=transform)\n",
    "train_loader = torch.utils.data.DataLoader(dataset1,batch_size=nsamples, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(dataset2,batch_size=nsamples, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a2e05c4b-31e7-4856-8151-91841410ef94",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MNIST_NN_Linear().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d8c5afa8-4e2c-44a9-92b8-07cdd39d004e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1:\n",
      "Imagens (data): torch.Size([50, 1, 28, 28])\n",
      "Rótulos (target): torch.Size([50])\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "for batch_idx, (data, target) in enumerate(train_loader):\n",
    "    print(f'Batch {batch_idx + 1}:')\n",
    "    print('Imagens (data):', data.size())\n",
    "    print('Rótulos (target):', target.size())\n",
    "\n",
    "\n",
    "    plt.imshow(data[0].squeeze(), cmap='gray')\n",
    "    plt.title(f'Rótulo: {target[0].item()}')\n",
    "    plt.show()\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c6777499-ed4c-4304-a544-90976b43c970",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 \n",
      " Loss: 2.3045494556427 \n",
      " Acurracy: 8.333333244081587e-05\n",
      "Train Epoch: 10 \n",
      " Loss: 2.280940532684326 \n",
      " Acurracy: 0.0019166666315868497\n",
      "Train Epoch: 20 \n",
      " Loss: 2.225053310394287 \n",
      " Acurracy: 0.005200000014156103\n",
      "Train Epoch: 30 \n",
      " Loss: 2.0907416343688965 \n",
      " Acurracy: 0.008899999782443047\n",
      "Train Epoch: 40 \n",
      " Loss: 2.05618953704834 \n",
      " Acurracy: 0.01283333357423544\n",
      "Train Epoch: 50 \n",
      " Loss: 1.9011667966842651 \n",
      " Acurracy: 0.017249999567866325\n",
      "Train Epoch: 60 \n",
      " Loss: 1.8284428119659424 \n",
      " Acurracy: 0.022483333945274353\n",
      "Train Epoch: 70 \n",
      " Loss: 1.8944923877716064 \n",
      " Acurracy: 0.027633333578705788\n",
      "Train Epoch: 80 \n",
      " Loss: 1.798567295074463 \n",
      " Acurracy: 0.032999999821186066\n",
      "Train Epoch: 90 \n",
      " Loss: 1.808791160583496 \n",
      " Acurracy: 0.03883333504199982\n",
      "Train Epoch: 100 \n",
      " Loss: 1.8441224098205566 \n",
      " Acurracy: 0.04488333314657211\n",
      "Train Epoch: 110 \n",
      " Loss: 1.7482385635375977 \n",
      " Acurracy: 0.05090000107884407\n",
      "Train Epoch: 120 \n",
      " Loss: 1.825496792793274 \n",
      " Acurracy: 0.057216666638851166\n",
      "Train Epoch: 130 \n",
      " Loss: 1.7063751220703125 \n",
      " Acurracy: 0.06391666829586029\n",
      "Train Epoch: 140 \n",
      " Loss: 1.6491721868515015 \n",
      " Acurracy: 0.07074999809265137\n",
      "Train Epoch: 150 \n",
      " Loss: 1.6946145296096802 \n",
      " Acurracy: 0.07769999653100967\n",
      "Train Epoch: 160 \n",
      " Loss: 1.6314010620117188 \n",
      " Acurracy: 0.08438333123922348\n",
      "Train Epoch: 170 \n",
      " Loss: 1.7143093347549438 \n",
      " Acurracy: 0.09099999815225601\n",
      "Train Epoch: 180 \n",
      " Loss: 1.805771827697754 \n",
      " Acurracy: 0.09778333455324173\n",
      "Train Epoch: 190 \n",
      " Loss: 1.709457516670227 \n",
      " Acurracy: 0.10463333129882812\n",
      "Train Epoch: 200 \n",
      " Loss: 1.6966891288757324 \n",
      " Acurracy: 0.1114666685461998\n",
      "Train Epoch: 210 \n",
      " Loss: 1.679947853088379 \n",
      " Acurracy: 0.11838333308696747\n",
      "Train Epoch: 220 \n",
      " Loss: 1.6491316556930542 \n",
      " Acurracy: 0.12573333084583282\n",
      "Train Epoch: 230 \n",
      " Loss: 1.6346635818481445 \n",
      " Acurracy: 0.13304999470710754\n",
      "Train Epoch: 240 \n",
      " Loss: 1.6353181600570679 \n",
      " Acurracy: 0.14036667346954346\n",
      "Train Epoch: 250 \n",
      " Loss: 1.5892601013183594 \n",
      " Acurracy: 0.14773333072662354\n",
      "Train Epoch: 260 \n",
      " Loss: 1.6409858465194702 \n",
      " Acurracy: 0.15521666407585144\n",
      "Train Epoch: 270 \n",
      " Loss: 1.5645564794540405 \n",
      " Acurracy: 0.16255000233650208\n",
      "Train Epoch: 280 \n",
      " Loss: 1.6080975532531738 \n",
      " Acurracy: 0.16985000669956207\n",
      "Train Epoch: 290 \n",
      " Loss: 1.5806230306625366 \n",
      " Acurracy: 0.1772666722536087\n",
      "Train Epoch: 300 \n",
      " Loss: 1.6105778217315674 \n",
      " Acurracy: 0.18451666831970215\n",
      "Train Epoch: 310 \n",
      " Loss: 1.6094659566879272 \n",
      " Acurracy: 0.19196666777133942\n",
      "Train Epoch: 320 \n",
      " Loss: 1.590192437171936 \n",
      " Acurracy: 0.19949999451637268\n",
      "Train Epoch: 330 \n",
      " Loss: 1.5776623487472534 \n",
      " Acurracy: 0.20688332617282867\n",
      "Train Epoch: 340 \n",
      " Loss: 1.5832654237747192 \n",
      " Acurracy: 0.21436665952205658\n",
      "Train Epoch: 350 \n",
      " Loss: 1.576784372329712 \n",
      " Acurracy: 0.22184999287128448\n",
      "Train Epoch: 360 \n",
      " Loss: 1.5642606019973755 \n",
      " Acurracy: 0.22914999723434448\n",
      "Train Epoch: 370 \n",
      " Loss: 1.5967566967010498 \n",
      " Acurracy: 0.2366333305835724\n",
      "Train Epoch: 380 \n",
      " Loss: 1.5690335035324097 \n",
      " Acurracy: 0.24406667053699493\n",
      "Train Epoch: 390 \n",
      " Loss: 1.564868450164795 \n",
      " Acurracy: 0.2515333294868469\n",
      "Train Epoch: 400 \n",
      " Loss: 1.5777074098587036 \n",
      " Acurracy: 0.25878334045410156\n",
      "Train Epoch: 410 \n",
      " Loss: 1.5777052640914917 \n",
      " Acurracy: 0.2662833333015442\n",
      "Train Epoch: 420 \n",
      " Loss: 1.5487074851989746 \n",
      " Acurracy: 0.27390000224113464\n",
      "Train Epoch: 430 \n",
      " Loss: 1.5681005716323853 \n",
      " Acurracy: 0.2813666760921478\n",
      "Train Epoch: 440 \n",
      " Loss: 1.5688388347625732 \n",
      " Acurracy: 0.2888999879360199\n",
      "Train Epoch: 450 \n",
      " Loss: 1.6056877374649048 \n",
      " Acurracy: 0.29651665687561035\n",
      "Train Epoch: 460 \n",
      " Loss: 1.5666714906692505 \n",
      " Acurracy: 0.30416667461395264\n",
      "Train Epoch: 470 \n",
      " Loss: 1.588952660560608 \n",
      " Acurracy: 0.31166666746139526\n",
      "Train Epoch: 480 \n",
      " Loss: 1.583958387374878 \n",
      " Acurracy: 0.31915000081062317\n",
      "Train Epoch: 490 \n",
      " Loss: 1.5181595087051392 \n",
      " Acurracy: 0.32678332924842834\n",
      "Train Epoch: 500 \n",
      " Loss: 1.513124942779541 \n",
      " Acurracy: 0.3344166576862335\n",
      "Train Epoch: 510 \n",
      " Loss: 1.5716761350631714 \n",
      " Acurracy: 0.3420499861240387\n",
      "Train Epoch: 520 \n",
      " Loss: 1.6369447708129883 \n",
      " Acurracy: 0.34948334097862244\n",
      "Train Epoch: 530 \n",
      " Loss: 1.5320020914077759 \n",
      " Acurracy: 0.3569166660308838\n",
      "Train Epoch: 540 \n",
      " Loss: 1.5604579448699951 \n",
      " Acurracy: 0.36453333497047424\n",
      "Train Epoch: 550 \n",
      " Loss: 1.5375257730484009 \n",
      " Acurracy: 0.3721500039100647\n",
      "Train Epoch: 560 \n",
      " Loss: 1.5645848512649536 \n",
      " Acurracy: 0.3796166777610779\n",
      "Train Epoch: 570 \n",
      " Loss: 1.4884659051895142 \n",
      " Acurracy: 0.38723334670066833\n",
      "Train Epoch: 580 \n",
      " Loss: 1.573434591293335 \n",
      " Acurracy: 0.3947833478450775\n",
      "Train Epoch: 590 \n",
      " Loss: 1.5281237363815308 \n",
      " Acurracy: 0.40228334069252014\n",
      "Train Epoch: 600 \n",
      " Loss: 1.5343836545944214 \n",
      " Acurracy: 0.4098833203315735\n",
      "Train Epoch: 610 \n",
      " Loss: 1.570888876914978 \n",
      " Acurracy: 0.4176333248615265\n",
      "Train Epoch: 620 \n",
      " Loss: 1.5347528457641602 \n",
      " Acurracy: 0.4252333343029022\n",
      "Train Epoch: 630 \n",
      " Loss: 1.5450644493103027 \n",
      " Acurracy: 0.4328833222389221\n",
      "Train Epoch: 640 \n",
      " Loss: 1.5507622957229614 \n",
      " Acurracy: 0.4405166804790497\n",
      "Train Epoch: 650 \n",
      " Loss: 1.5379302501678467 \n",
      " Acurracy: 0.44796666502952576\n",
      "Train Epoch: 660 \n",
      " Loss: 1.5734374523162842 \n",
      " Acurracy: 0.45570001006126404\n",
      "Train Epoch: 670 \n",
      " Loss: 1.5334045886993408 \n",
      " Acurracy: 0.46326667070388794\n",
      "Train Epoch: 680 \n",
      " Loss: 1.5451278686523438 \n",
      " Acurracy: 0.47083333134651184\n",
      "Train Epoch: 690 \n",
      " Loss: 1.554579496383667 \n",
      " Acurracy: 0.4786166548728943\n",
      "Train Epoch: 700 \n",
      " Loss: 1.619972825050354 \n",
      " Acurracy: 0.48633334040641785\n",
      "Train Epoch: 710 \n",
      " Loss: 1.5485540628433228 \n",
      " Acurracy: 0.4940166771411896\n",
      "Train Epoch: 720 \n",
      " Loss: 1.540921926498413 \n",
      " Acurracy: 0.5017333626747131\n",
      "Train Epoch: 730 \n",
      " Loss: 1.527084469795227 \n",
      " Acurracy: 0.5094333291053772\n",
      "Train Epoch: 740 \n",
      " Loss: 1.588363766670227 \n",
      " Acurracy: 0.5168833136558533\n",
      "Train Epoch: 750 \n",
      " Loss: 1.4902952909469604 \n",
      " Acurracy: 0.5244500041007996\n",
      "Train Epoch: 760 \n",
      " Loss: 1.5313481092453003 \n",
      " Acurracy: 0.5321166515350342\n",
      "Train Epoch: 770 \n",
      " Loss: 1.5464061498641968 \n",
      " Acurracy: 0.5395833253860474\n",
      "Train Epoch: 780 \n",
      " Loss: 1.602754831314087 \n",
      " Acurracy: 0.5471500158309937\n",
      "Train Epoch: 790 \n",
      " Loss: 1.5117051601409912 \n",
      " Acurracy: 0.5547500252723694\n",
      "Train Epoch: 800 \n",
      " Loss: 1.5285344123840332 \n",
      " Acurracy: 0.5624666810035706\n",
      "Train Epoch: 810 \n",
      " Loss: 1.578487515449524 \n",
      " Acurracy: 0.5700500011444092\n",
      "Train Epoch: 820 \n",
      " Loss: 1.5376754999160767 \n",
      " Acurracy: 0.5776166915893555\n",
      "Train Epoch: 830 \n",
      " Loss: 1.4935510158538818 \n",
      " Acurracy: 0.5852500200271606\n",
      "Train Epoch: 840 \n",
      " Loss: 1.5453273057937622 \n",
      " Acurracy: 0.5929833054542542\n",
      "Train Epoch: 850 \n",
      " Loss: 1.5211642980575562 \n",
      " Acurracy: 0.600600004196167\n",
      "Train Epoch: 860 \n",
      " Loss: 1.571335792541504 \n",
      " Acurracy: 0.6082000136375427\n",
      "Train Epoch: 870 \n",
      " Loss: 1.5943490266799927 \n",
      " Acurracy: 0.6160333156585693\n",
      "Train Epoch: 880 \n",
      " Loss: 1.5509182214736938 \n",
      " Acurracy: 0.6236166954040527\n",
      "Train Epoch: 890 \n",
      " Loss: 1.5688276290893555 \n",
      " Acurracy: 0.6311500072479248\n",
      "Train Epoch: 900 \n",
      " Loss: 1.5351239442825317 \n",
      " Acurracy: 0.6386333107948303\n",
      "Train Epoch: 910 \n",
      " Loss: 1.5908114910125732 \n",
      " Acurracy: 0.6463333368301392\n",
      "Train Epoch: 920 \n",
      " Loss: 1.5343194007873535 \n",
      " Acurracy: 0.6539333462715149\n",
      "Train Epoch: 930 \n",
      " Loss: 1.5016913414001465 \n",
      " Acurracy: 0.661466658115387\n",
      "Train Epoch: 940 \n",
      " Loss: 1.5266865491867065 \n",
      " Acurracy: 0.6690000295639038\n",
      "Train Epoch: 950 \n",
      " Loss: 1.5662633180618286 \n",
      " Acurracy: 0.6765999794006348\n",
      "Train Epoch: 960 \n",
      " Loss: 1.5645884275436401 \n",
      " Acurracy: 0.6843166947364807\n",
      "Train Epoch: 970 \n",
      " Loss: 1.5230917930603027 \n",
      " Acurracy: 0.6920666694641113\n",
      "Train Epoch: 980 \n",
      " Loss: 1.555382251739502 \n",
      " Acurracy: 0.6998000144958496\n",
      "Train Epoch: 990 \n",
      " Loss: 1.5301659107208252 \n",
      " Acurracy: 0.7076500058174133\n",
      "Train Epoch: 1000 \n",
      " Loss: 1.5178420543670654 \n",
      " Acurracy: 0.715233325958252\n",
      "Train Epoch: 1010 \n",
      " Loss: 1.5163967609405518 \n",
      " Acurracy: 0.7230166792869568\n",
      "Train Epoch: 1020 \n",
      " Loss: 1.502084493637085 \n",
      " Acurracy: 0.730816662311554\n",
      "Train Epoch: 1030 \n",
      " Loss: 1.524015188217163 \n",
      " Acurracy: 0.7385666370391846\n",
      "Train Epoch: 1040 \n",
      " Loss: 1.6065664291381836 \n",
      " Acurracy: 0.7461166381835938\n",
      "Train Epoch: 1050 \n",
      " Loss: 1.534327507019043 \n",
      " Acurracy: 0.7538666725158691\n",
      "Train Epoch: 1060 \n",
      " Loss: 1.5452237129211426 \n",
      " Acurracy: 0.7615500092506409\n",
      "Train Epoch: 1070 \n",
      " Loss: 1.5667939186096191 \n",
      " Acurracy: 0.7692499756813049\n",
      "Train Epoch: 1080 \n",
      " Loss: 1.5276310443878174 \n",
      " Acurracy: 0.7769166827201843\n",
      "Train Epoch: 1090 \n",
      " Loss: 1.550052285194397 \n",
      " Acurracy: 0.784683346748352\n",
      "Train Epoch: 1100 \n",
      " Loss: 1.557023048400879 \n",
      " Acurracy: 0.7922499775886536\n",
      "Train Epoch: 1110 \n",
      " Loss: 1.5782606601715088 \n",
      " Acurracy: 0.7997999787330627\n",
      "Train Epoch: 1120 \n",
      " Loss: 1.5974700450897217 \n",
      " Acurracy: 0.8074333071708679\n",
      "Train Epoch: 1130 \n",
      " Loss: 1.5307731628417969 \n",
      " Acurracy: 0.8151166439056396\n",
      "Train Epoch: 1140 \n",
      " Loss: 1.5120360851287842 \n",
      " Acurracy: 0.822950005531311\n",
      "Train Epoch: 1150 \n",
      " Loss: 1.5043303966522217 \n",
      " Acurracy: 0.8308333158493042\n",
      "Train Epoch: 1160 \n",
      " Loss: 1.5354599952697754 \n",
      " Acurracy: 0.8385666608810425\n",
      "Train Epoch: 1170 \n",
      " Loss: 1.6095712184906006 \n",
      " Acurracy: 0.8463166952133179\n",
      "Train Epoch: 1180 \n",
      " Loss: 1.533846378326416 \n",
      " Acurracy: 0.8539666533470154\n",
      "Train Epoch: 1190 \n",
      " Loss: 1.5999667644500732 \n",
      " Acurracy: 0.8616833090782166\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "train(model,device,train_loader,optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2180ff7a-903f-4778-b8d1-70a2e2ebe00a",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5f6229fb-a10c-437f-b67f-56bfa6a9ffdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.030752501451969148 \n",
      " Acurracy 0.9294\n"
     ]
    }
   ],
   "source": [
    "test(model,device,test_loader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
